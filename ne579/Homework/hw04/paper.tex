\documentclass{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[alsoload=synchem,load=named]{siunitx}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs, tabularx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{physics}
\usepackage[citestyle=ieee,sorting=none,bibencoding=utf8,backend=biber]{biblatex}

\graphicspath{{images/}}
\bibliography{bibliography}

\author{J.R. Powers-Luhn}
\title{Principal Component Analysis of Body Composition Measurements}
\date{October 2nd, 2018}

\DeclareSIUnit\year{yr}
\DeclareSIUnit\inch{in}

\begin{document}
\maketitle

\begin{abstract}

The generation of physical models suffers from the presence of noise in the measured quantities. Simple models such as linear regression assume that the input parameters are noiseless and further assume that they are linearly independent. Real-life data rarely satisfies the latter assumption and never satisfies the former. A technique for compensating for this, principal component analysis, is examined and applied to body composition measurements in an attempt to predict body fat percentage. The dataset is transformed into a principal component space of orthogonal components and these are used to generate regression models of the data. This method underperformed naive linear regressions with a root mean squared error of \num{5.10}\%.

\end{abstract}

\section{Introduction}

With the advent of automated data collection systems, datasets have become both deeper (with more granular data recorded at the second or sub-second level thanks to digital storage) and wider (with hundreds or thousands of sensors querying a system at once). These sensors are still subject to noise and may measure correlated quantities. Choosing which combination of sensors to include as inputs in a model scales in proportion to the number of sensors squared, making the creation of useful models grow rapidly more complex. This can be addressed by dimensionality reduction techniques such as principal component analysis (PCA). 

Principal component analysis is a linear transformation of a dataset. The measured input variables are used to generate an orthogonal spanning set of principal components that covers the input space. Since the transformation is linear it can be applied to future additions to the dataset without updating the parameters. The columns of the transformation matrix are referred to as principal components; the products of the input measurements and the principal component transformation matrix are referred to as loading vectors or loadings. Principal component analysis has the additional design benefit of ordering each component based on the variance in the input data. This allows for dimensionality reduction with little loss of variance. Further, the rejection of low-variance components in the transformed space allows for more stability in the model by reducing the condition number of the matrix of input measurements, $\frac{\lambda_{max}}{\lambda_{min}}$.

A final advantage of PCA is apparent if the assumption is made that the variance in the measured data exceeds the variance introduced by the noise in the measurements. This means that be rejecting loadings that correspond to low-variance principal components, the effect of noise on regression models can be reduced.

\section{Methodology} 

A dataset consisting of NUMBER samples with fifteen measurements recorded for each participant was obtained \cite{Penrose1985}. The first fourteen measurements were age, weight, adiposity index, and various circumference measurements. The last measurement was body fat percentage. Since many of the circumference measurements showed a high correlation (e.g. wrist and forearm circumference) principal component analysis was performed in order to isolate the varying components. The data set was split into training, testing, and validation sets by randomly sampling rows such that 70\% of the data was placed in the training set and 15\% each placed in the testing and validation set. The random number generator used to sample the rows was saved for reproduceability. 

Principal component analysis consists of a linear transformation from the measured units (in this case, a fourteen-dimensional space spanned by non-orthogonal measurements) into a new space where the unit vectors are orthogonal to each other. This process has the additional property that the spanning set that is so derived is ordered from highest to lowest variance from the original data. If the noise in a measurement can be assumed to be smaller in variance than the measured quantities, this means that the noise will be isolated to the later vectors.

In order to derive the new spanning set, the covariance matrix of the input measurements is taken. This is then decomposed into equation \ref{eq:eigenvalue_solution}:

\begin{align}
	\mathbf{C} \propto \mathbf{X}^T \mathbf{X} = \mathbf{W} \mathbf{\Lambda} \mathbf{W}^T
	\label{eq:eigenvalue_solution}
\end{align}

where \textbf{X} is the original sample (where each column represents a measurement and each row represents a sample), $\mathbf{\Lambda}$ represents a diagonal matrix of the eigenvalues, and $\mathbf{W}$ represents the matrix which transforms the rows from \textbf{X} into the new space. This is performed in such a fashion that the eigenvalue/eigenvector pair selected is the one that also captures the maximum amount of variance from the original data. This gives the first principal component vector. Subsequent loadings must also satisfy the condition that they be orthogonal to the loadings calculated before them. This is satisfied by first removing the already-calculated loadings from $\mathbf{X}$, then repeating the process on the new measured value (as in equations \ref{eq:later_pc1} and \ref{eq:later_pc2}). This can be repeated up to the minimum of the number of columns or the number of rows in $\mathbf{X}$. These vectors can be assembled into the matrix $\mathbf{W}$ which can transform measurement vectors from $\mathbf{X}$ into the loadings. The eigenvalues are also proportional to the amount of variance captured by the corresponding component. These eigenvalues can be normalized (equation \ref{eq:eigen_norm}) to determine the fraction of the total variance carried by each principal component.

\begin{align}
	\mathbf{X}_k &= \mathbf{X} - \sum_{i=0}^{k-1} \mathbf{X} \mathbf{w}_i \mathbf{w}_i^T \label{eq:later_pc1}\\
	\mathbf{w}_k &= \operatornamewithlimits{argmin}\limits_{||\mathbf{w}_k||} \frac{\mathbf{w}^T \mathbf{X}_k^T \mathbf{X}_k \mathbf{w}}{\mathbf{w}^T \mathbf{w}}
	\label{eq:later_pc2}
\end{align}

\begin{equation}
	\left| \lambda_i \right| = \frac{\lambda_i}{\sum_{i} \lambda_i}
	\label{eq:eigen_norm}
\end{equation}

Because the loadings are orthogonal to each other, it is possible to exclude individual columns from the transformed. This is equivalent to excluding the input from an individual sensor from a regression but with the added benefit that later loadings correspond to lower variance. If the assumption that the noise is lower variance than the measurements, this means that noise can be excluded simply by eliminating the low-variance vectors. In order to explore this, principal components were excluded based on three different parameters. First components which cumulatively captured at least 90\% of the variance (calculated as the first $n$ loadings such that $\operatornamewithlimits{argmin}\limits_{n} \sum_{i=0}^{n-1} \left| \lambda_i \right| \geq 0.9$. Second, components representing at least 1\% of the variance from the original sample ($\left| \lambda_i \right| \geq 0.01$) were isolated. Finally, principal components that were determined to be correlated with the output variable (body fat percentage) were isolated and used.

\section{Results}

The input data was transformed into the PCA space. The first four principal components were visually examined to see of what they consisted (see figure \ref{fig:first_four_pca}). All inputs were included in the first component except age--the variable least correlated with the other inputs. All other variables were included reduced by a factor of approximately \numrange{0.2}{0.3} except height, which was the second lest correlated. Age and height are more represented in PCA 1 and 2 (PCA's were numbered starting with 0). 

\begin{centering}
\begin{figure}
\centering
\begin{center}
	\includegraphics[width=0.5\textwidth]{first_four_pca}
	\caption{The first four principal components, capturing approximately 90\% of the variance. Age does not contribute to the first principal component, likely because it does not covary with the other measured values.\label{fig:first_four_pca}}
\end{center}
\end{figure}
\end{centering}

A regression was performed against the loadings corresponding to the first five PC's. This corresponded to the PC's that carried \textgreater90\% of the variance from the original sample, as shown in figure \ref{fig:cumulative_variance}. The performance of this model was the worst of all the models tested with a root mean squared error (RMSE) on the test set of \num{5.03}.

\begin{centering}
\begin{figure}
\centering
\begin{center}
	\includegraphics[width=0.5\textwidth]{cumulative_variance}
	\caption{Cumulative variance captured by the first $n$ principal components. Note that the plot starts at 0.65 and has a negative second derivative. This shows that subsequent principal components capture less and less of the variance.\label{fig:cumulative_variance}}
\end{center}
\end{figure}
\end{centering}

Next, a regression model was generated that used all principal components with eigenvalues corresponding to \textgreater1\% of the variance. This included PC's \numrange{0}{8} (figure \ref{fig:scaled_eigen}).. This model performed better, with a RMSE of \num{4.50}. It seems that PC's \numrange{5}{8} contain information required to calculate body fat percentage so that including the loadings corresponding to these components improved the performance of the model.

\begin{centering}
\begin{figure}
\centering
\begin{center}
	\includegraphics[width=0.5\textwidth]{fractional_variance_explained}
	\caption{Fraction of variance captured by each principal component. Each subsequent component captures less variance than its predecessor, necessitating a log scale. The red dotted line represents the 1\% variance threshold.\label{fig:scaled_eigen}}
\end{center}
\end{figure}
\end{centering}

Loadings that were correlated with the output variable were determined by calculating the absolute value of the correlation coefficient between each loading and body fat percentage (see figure \ref{fig:pca_correlation}). Linear regression against these components showed degredation over the previous method attempted, with a test set RMSE of \num{4.92}. When the next most correlated loading column was included in the regression model performance improved, reaching a RMSE of \num{4.24}. In each case consideration was given to the stability of the solution by examining the condition number of the input (see figure \ref{fig:condition_number}) to verify that it did not significantly exceed the threshold value of 100. An examination of plots of each loading column vs. body fat percentage showed no evidence of non-linear relationships so models incorporating these elements were not attempted.

\begin{centering}
\begin{figure}
\centering
\begin{center}
	\includegraphics[width=0.5\textwidth]{pca_correlation}
	\caption{Correlation between each loading column and body fat percentage. No loading reached the threshold of "strongly correlated" (\textgreater0.7) and only the first two were correlated (\textgreater0.3).\label{fig:pca_correlation}}
\end{center}
\end{figure}
\end{centering}

\begin{centering}
\begin{figure}
\centering
\begin{center}
	\includegraphics[width=0.5\textwidth]{condition_number}
	\caption{Condition number of the input matrix vs number of principal components included. The rule of thumb threshold value of 100 is denoted in red.\label{fig:condition_number}}
\end{center}
\end{figure}
\end{centering}

Finally, a principal component regression of all components was performed to test that performance. This was not expected to perform well, as it was mathematically equivalent to a linear regression using the original inputs. This model, however, outperformed all of the PCR models on the test set, with a RMSE of \num{3.83}.

Based on these results, a regression of the most correlated loadings against body fat percentage was selected as the best model. Based on the visualization in figure \ref{fig:first_four_pca}, all measured values were present in the first two loading columns, implying that including additional columns introduces the potential to overfit the data.

The selected model had an error of \num{5.10} on the validation set. This was worse than the error on the validation set for a naive linear regression on all columns in the original data set (\num{4.19}).

\begin{centering}
\begin{table}
\caption{Results of Principle Component Regressions (test set)\label{tab:results}}
\begin{tabular}{lr}
\toprule
                                Model &      RMSE \\
\midrule
        PCR Capturing 90\% of Variance &  5.025354 \\
 PCR of loadings with \textgreater1\% of Variance &  4.500349 \\
    PCR of correlated loadings (\textgreater0.3) &  4.235354 \\
    PCR of correlated loadings (\textgreater0.2) &  4.916291 \\
                  PCR of all loadings &  3.832659 \\
\bottomrule
\end{tabular}
\end{table}
\end{centering}

\section{Conclusions}

While principal component analysis and principal component regression are promising tools for future analysis, they are not an automatically superior modeling technique. 

%\nocite{*}
\printbibliography

\onecolumn
\section{Appendix}
Python code used to perform calculations and generate graphics.
\lstset{frame=single}
\lstinputlisting[language=python]{appendix/appendix.py}

\end{document}